{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "753b4542",
   "metadata": {},
   "source": [
    "# Predicción de disponibilidad de agua (Acea Water Prediction) — Enfoque de Series de Tiempo\n",
    "\n",
    "En este notebook se construye un **flujo replicable** para entrenar modelos de predicción en **nueve cuerpos de agua** (acuíferos, manantiales, río y lago) usando un enfoque de **series temporales**.\n",
    "\n",
    "**Idea clave:** cada dataset es independiente y tiene variables/targets diferentes; por eso, se entrena un modelo por *waterbody* (y se organiza por categoría).  \n",
    "El objetivo es que el mismo procedimiento pueda aplicarse a nuevos cuerpos de agua que tengan columnas similares.\n",
    "\n",
    "---\n",
    "\n",
    "## ¿Por qué NO conviene unir toda la data?\n",
    "\n",
    "- **Diferentes variables**: no todas las columnas existen en todos los cuerpos de agua (por ejemplo, un lago tiene `Lake_Level`, un río tiene `Hydrometry_*`).\n",
    "- **Diferentes dinámicas físicas**: un acuífero responde a lluvia/temperatura con retrasos distintos a un río o a un lago.\n",
    "- **Frecuencias y ventanas**: aun con fechas diarias, los patrones y escalas son distintos; unirlos puede introducir sesgos y ruido.\n",
    "- **Objetivos múltiples y no comparables**: algunos cuerpos tienen varios pozos (targets múltiples) y otros uno solo.\n",
    "\n",
    "En lugar de mezclar todo, **se estandariza el pipeline** (limpieza → features temporales → lags → split temporal → modelo → métricas → guardado).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8064d8a9",
   "metadata": {},
   "source": [
    "## 1) Configuración e importaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b98cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se importan librerías estándar para análisis, modelado y guardado.\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9984714",
   "metadata": {},
   "source": [
    "## 2) Carga de datasets (9 waterbodies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e8b673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se define el mapeo nombre -> ruta de archivo (asumiendo que los CSV están en el mismo directorio del notebook o en /mnt/data).\n",
    "DATASETS = {\n",
    "    \"Aquifer_Auser\": \"/mnt/data/Aquifer_Auser.csv\",\n",
    "    \"Aquifer_Petrignano\": \"/mnt/data/Aquifer_Petrignano.csv\",\n",
    "    \"Aquifer_Doganella\": \"/mnt/data/Aquifer_Doganella.csv\",\n",
    "    \"Aquifer_Luco\": \"/mnt/data/Aquifer_Luco.csv\",\n",
    "    \"Water_Spring_Amiata\": \"/mnt/data/Water_Spring_Amiata.csv\",\n",
    "    \"Water_Spring_Madonna_di_Canneto\": \"/mnt/data/Water_Spring_Madonna_di_Canneto.csv\",\n",
    "    \"Water_Spring_Lupa\": \"/mnt/data/Water_Spring_Lupa.csv\",\n",
    "    \"River_Arno\": \"/mnt/data/River_Arno.csv\",\n",
    "    \"Lake_Bilancino\": \"/mnt/data/Lake_Bilancino.csv\",\n",
    "}\n",
    "\n",
    "# Se cargan los CSV en un diccionario para trabajar de forma homogénea.\n",
    "raw = {name: pd.read_csv(path) for name, path in DATASETS.items()}\n",
    "\n",
    "# Se muestra un resumen rápido.\n",
    "for name, df in raw.items():\n",
    "    print(f\"{name:28s} -> shape={df.shape}, columnas={len(df.columns)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d950438",
   "metadata": {},
   "source": [
    "## 3) Utilidades para series temporales (features temporales + lags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9419959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se implementan funciones para:\n",
    "# - convertir la columna Date a datetime\n",
    "# - ordenar por fecha\n",
    "# - crear variables temporales (mes, día del año, senos/cosenos)\n",
    "# - crear lags y medias móviles de variables exógenas (por ejemplo Rainfall, Temperature)\n",
    "# - construir X, y de forma supervisada para regresión (multioutput cuando aplique)\n",
    "\n",
    "def detect_date_column(df: pd.DataFrame) -> str:\n",
    "    # Se detecta la columna de fecha típica usada en estos datasets.\n",
    "    candidates = [c for c in df.columns if c.lower() in (\"date\", \"datetime\", \"time\", \"timestamp\")]\n",
    "    if not candidates:\n",
    "        raise ValueError(\"No se encontró columna de fecha (Date/datetime/time/timestamp).\")\n",
    "    return candidates[0]\n",
    "\n",
    "def infer_targets(df: pd.DataFrame) -> list[str]:\n",
    "    # Se infieren targets siguiendo la convención del challenge.\n",
    "    prefixes = (\"Depth_to_Groundwater\", \"Flow_Rate\", \"Hydrometry\", \"Lake_Level\")\n",
    "    targets = [c for c in df.columns if c.startswith(prefixes)]\n",
    "    # Se evita que la fecha sea tomada como target por error.\n",
    "    return targets\n",
    "\n",
    "def add_time_features(df: pd.DataFrame, date_col: str) -> pd.DataFrame:\n",
    "    # Se construyen features temporales cíclicas para capturar estacionalidad.\n",
    "    out = df.copy()\n",
    "    out[\"year\"] = out[date_col].dt.year\n",
    "    out[\"month\"] = out[date_col].dt.month\n",
    "    out[\"dayofyear\"] = out[date_col].dt.dayofyear\n",
    "\n",
    "    # Se codifican variables cíclicas (sen/cos) para mes y día del año.\n",
    "    out[\"month_sin\"] = np.sin(2 * np.pi * out[\"month\"] / 12)\n",
    "    out[\"month_cos\"] = np.cos(2 * np.pi * out[\"month\"] / 12)\n",
    "    out[\"doy_sin\"] = np.sin(2 * np.pi * out[\"dayofyear\"] / 365.25)\n",
    "    out[\"doy_cos\"] = np.cos(2 * np.pi * out[\"dayofyear\"] / 365.25)\n",
    "    return out\n",
    "\n",
    "def add_lag_features(df: pd.DataFrame, cols: list[str], lags=(1, 7, 14, 30), rolls=(7, 30)) -> pd.DataFrame:\n",
    "    # Se agregan lags y medias móviles para capturar retrasos (efecto lluvia/temperatura días después).\n",
    "    out = df.copy()\n",
    "    for c in cols:\n",
    "        for L in lags:\n",
    "            out[f\"{c}_lag{L}\"] = out[c].shift(L)\n",
    "        for R in rolls:\n",
    "            out[f\"{c}_rollmean{R}\"] = out[c].rolling(R).mean()\n",
    "    return out\n",
    "\n",
    "def temporal_train_test_split(df: pd.DataFrame, test_size: float = 0.2):\n",
    "    # Se realiza un split temporal: el último 20% se usa como test (evita leakage).\n",
    "    n = len(df)\n",
    "    cut = int(np.floor((1 - test_size) * n))\n",
    "    train_df = df.iloc[:cut].copy()\n",
    "    test_df = df.iloc[cut:].copy()\n",
    "    return train_df, test_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371fa632",
   "metadata": {},
   "source": [
    "## 4) Función de entrenamiento (modelo supervisado a partir de lags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02b44c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se define una función genérica para entrenar un modelo por waterbody:\n",
    "# - prepara dataframe temporal\n",
    "# - crea features\n",
    "# - construye X/y\n",
    "# - entrena un regressor multioutput\n",
    "# - evalúa con MAE y RMSE\n",
    "# - retorna modelo + métricas + predicciones\n",
    "\n",
    "def train_time_series_regressor(df_raw: pd.DataFrame, name: str, test_size: float = 0.2):\n",
    "    df = df_raw.copy()\n",
    "\n",
    "    # Se parsea y ordena por fecha.\n",
    "    date_col = detect_date_column(df)\n",
    "    df[date_col] = pd.to_datetime(df[date_col])\n",
    "    df = df.sort_values(date_col).reset_index(drop=True)\n",
    "\n",
    "    # Se identifican targets (columnas a predecir).\n",
    "    target_cols = infer_targets(df)\n",
    "    if len(target_cols) == 0:\n",
    "        raise ValueError(f\"No se detectaron targets en {name}. Revisar columnas.\")\n",
    "\n",
    "    # Se seleccionan exógenas candidatas: todas las numéricas excepto targets.\n",
    "    # (Se excluye fecha y se intenta evitar fuga: no se usan targets como features).\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    exog_cols = [c for c in numeric_cols if c not in target_cols]\n",
    "\n",
    "    # Se agregan features temporales.\n",
    "    df_feat = add_time_features(df, date_col=date_col)\n",
    "\n",
    "    # Se agregan lags/rollings para exógenas (si existen).\n",
    "    if len(exog_cols) > 0:\n",
    "        df_feat = add_lag_features(df_feat, cols=exog_cols, lags=(1,7,14,30), rolls=(7,30))\n",
    "\n",
    "    # Se eliminan filas con NaN producidas por lags/rolling (normalmente al inicio).\n",
    "    df_feat = df_feat.dropna().reset_index(drop=True)\n",
    "\n",
    "    # Se construyen X e y.\n",
    "    y = df_feat[target_cols].copy()\n",
    "    X = df_feat.drop(columns=[date_col] + target_cols)\n",
    "\n",
    "    # Se hace split temporal.\n",
    "    train_df, test_df = temporal_train_test_split(df_feat, test_size=test_size)\n",
    "\n",
    "    y_train = train_df[target_cols]\n",
    "    X_train = train_df.drop(columns=[date_col] + target_cols)\n",
    "\n",
    "    y_test = test_df[target_cols]\n",
    "    X_test = test_df.drop(columns=[date_col] + target_cols)\n",
    "\n",
    "    # Se define un pipeline robusto: imputación + escalado + modelo.\n",
    "    # HistGradientBoostingRegressor se usa por estabilidad y buen desempeño en tabular.\n",
    "    base = HistGradientBoostingRegressor(random_state=42)\n",
    "    model = MultiOutputRegressor(base)\n",
    "\n",
    "    pipe = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"model\", model)\n",
    "    ])\n",
    "\n",
    "    # Se entrena el modelo.\n",
    "    pipe.fit(X_train, y_train)\n",
    "\n",
    "    # Se predice en test.\n",
    "    y_pred = pipe.predict(X_test)\n",
    "    y_pred = pd.DataFrame(y_pred, columns=target_cols, index=y_test.index)\n",
    "\n",
    "    # Se calculan métricas por target y promedio.\n",
    "    metrics = []\n",
    "    for t in target_cols:\n",
    "        mae = mean_absolute_error(y_test[t], y_pred[t])\n",
    "        rmse = np.sqrt(mean_squared_error(y_test[t], y_pred[t]))\n",
    "        metrics.append({\"waterbody\": name, \"target\": t, \"MAE\": mae, \"RMSE\": rmse})\n",
    "    metrics_df = pd.DataFrame(metrics)\n",
    "\n",
    "    # Se arma una salida completa para guardado/reproducibilidad.\n",
    "    artifact = {\n",
    "        \"waterbody\": name,\n",
    "        \"date_col\": date_col,\n",
    "        \"targets\": target_cols,\n",
    "        \"exog_cols\": exog_cols,\n",
    "        \"feature_columns\": X_train.columns.tolist(),\n",
    "        \"pipeline\": pipe\n",
    "    }\n",
    "    return artifact, metrics_df, (train_df, test_df, y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15c652e",
   "metadata": {},
   "source": [
    "## 5) Entrenamiento por categoría (acuíferos, manantiales, río, lago)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683893c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se entrena un modelo por dataset y se consolidan métricas.\n",
    "artifacts = {}\n",
    "all_metrics = []\n",
    "\n",
    "for name, df in raw.items():\n",
    "    print(f\"Entrenando: {name}\")\n",
    "    artifact, metrics_df, pack = train_time_series_regressor(df, name=name, test_size=0.2)\n",
    "    artifacts[name] = {\"artifact\": artifact, \"pack\": pack}\n",
    "    all_metrics.append(metrics_df)\n",
    "\n",
    "metrics_table = pd.concat(all_metrics, ignore_index=True)\n",
    "metrics_table.sort_values([\"waterbody\", \"target\"]).head(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7420ea3",
   "metadata": {},
   "source": [
    "## 6) Visualización: real vs predicho (ejemplo por dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3de426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se grafica un ejemplo por waterbody (primer target detectado) para validar visualmente la calidad del ajuste.\n",
    "def plot_real_vs_pred(name: str):\n",
    "    train_df, test_df, y_test, y_pred = artifacts[name][\"pack\"]\n",
    "    t = artifacts[name][\"artifact\"][\"targets\"][0]\n",
    "    date_col = artifacts[name][\"artifact\"][\"date_col\"]\n",
    "\n",
    "    # Se construye una serie temporal con fechas de test.\n",
    "    dates_test = test_df[date_col].iloc[y_test.index]\n",
    "\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.plot(dates_test, y_test[t].values, label=\"Real\")\n",
    "    plt.plot(dates_test, y_pred[t].values, label=\"Predicho\")\n",
    "    plt.title(f\"{name} — Real vs Predicho ({t})\")\n",
    "    plt.xlabel(\"Fecha\")\n",
    "    plt.ylabel(t)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "for name in raw.keys():\n",
    "    plot_real_vs_pred(name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aea2f7d",
   "metadata": {},
   "source": [
    "## 7) Resumen de métricas (MAE/RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56032724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se resume el desempeño promedio por waterbody (promedio de targets).\n",
    "summary_metrics = (\n",
    "    metrics_table\n",
    "    .groupby(\"waterbody\")[[\"MAE\",\"RMSE\"]]\n",
    "    .mean()\n",
    "    .sort_values(\"RMSE\")\n",
    ")\n",
    "summary_metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53268ebf",
   "metadata": {},
   "source": [
    "## 8) Guardado de modelos con joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c284bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se guardan los modelos (pipeline + metadata) con joblib en la carpeta models/.\n",
    "models_dir = Path(\"models\")\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "\n",
    "for name, obj in artifacts.items():\n",
    "    artifact = obj[\"artifact\"]\n",
    "    out_path = models_dir / f\"{name}_model.joblib\"\n",
    "    joblib.dump(artifact, out_path)\n",
    "    print(\"✅ Modelo guardado en:\", out_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54cf6407",
   "metadata": {},
   "source": [
    "## 9) Cómo reutilizar un modelo guardado\n",
    "\n",
    "Para usar un modelo guardado:\n",
    "\n",
    "1. Se carga el artefacto con `joblib.load`.\n",
    "2. Se prepara un dataframe nuevo con la misma lógica (fecha → features temporales + lags).\n",
    "3. Se hace `pipeline.predict(X_nuevo)`.\n",
    "\n",
    "En producción, lo ideal es encapsular la preparación de features en una función compartida para garantizar consistencia.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35e5f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo: cargar un modelo y predecir sobre las últimas filas del mismo dataset\n",
    "example_name = \"River_Arno\"\n",
    "loaded = joblib.load(Path(\"models\") / f\"{example_name}_model.joblib\")\n",
    "\n",
    "print(\"Waterbody:\", loaded[\"waterbody\"])\n",
    "print(\"Targets:\", loaded[\"targets\"][:5], \"...\")\n",
    "print(\"N_features:\", len(loaded[\"feature_columns\"]))\n",
    "\n",
    "# Se rehace el feature engineering sobre el dataset original para predecir.\n",
    "df_raw = raw[example_name].copy()\n",
    "date_col = loaded[\"date_col\"]\n",
    "df_raw[date_col] = pd.to_datetime(df_raw[date_col])\n",
    "df_raw = df_raw.sort_values(date_col).reset_index(drop=True)\n",
    "\n",
    "# Se replican los pasos de features.\n",
    "df_feat = add_time_features(df_raw, date_col=date_col)\n",
    "if len(loaded[\"exog_cols\"]) > 0:\n",
    "    df_feat = add_lag_features(df_feat, cols=loaded[\"exog_cols\"], lags=(1,7,14,30), rolls=(7,30))\n",
    "df_feat = df_feat.dropna().reset_index(drop=True)\n",
    "\n",
    "# Se arma X con las mismas columnas usadas en entrenamiento.\n",
    "X = df_feat.drop(columns=[date_col] + loaded[\"targets\"])\n",
    "X = X[loaded[\"feature_columns\"]]\n",
    "\n",
    "# Se predicen las últimas 5 observaciones.\n",
    "pred = loaded[\"pipeline\"].predict(X.tail(5))\n",
    "pred = pd.DataFrame(pred, columns=loaded[\"targets\"])\n",
    "pred\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
